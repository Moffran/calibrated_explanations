{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015a50a4",
   "metadata": {},
   "source": [
    "This notebook demonstrates the **new policy-based** behavior first (no use of `predict_reject`).\n",
    "At the end we show the legacy `predict_reject` workflow with a clear note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad1318ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported from: C:\\Users\\loftuw\\Documents\\Github\\kristinebergs-calibrated_explanations\\src\\calibrated_explanations\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ensure local src on path\n",
    "repo_root = Path.cwd()\n",
    "if not (repo_root / \"src\" / \"calibrated_explanations\").exists():\n",
    "    if (repo_root.parent / \"src\" / \"calibrated_explanations\").exists():\n",
    "        repo_root = repo_root.parent\n",
    "src_path = str(repo_root / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "import calibrated_explanations as ce_pkg\n",
    "from calibrated_explanations import CalibratedExplainer\n",
    "\n",
    "print(\"Imported from:\", Path(ce_pkg.__file__).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74dcdb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConformalClassifier(fitted=True, mondrian=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data and model\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_proper, X_cal, y_proper, y_cal = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42\n",
    ")\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_proper, y_proper)\n",
    "ce = CalibratedExplainer(model, X_cal, y_cal)\n",
    "ce.initialize_reject_learner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8776813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper summarizer tuned for explain_factual outputs\n",
    "def _summarize_result(result, *, label: str):\n",
    "    is_reject_result = hasattr(result, \"policy\") and hasattr(result, \"rejected\")\n",
    "    print(\"\\n---\", label, \"---\")\n",
    "    print(\"type:\", type(result))\n",
    "    if not is_reject_result:\n",
    "        if hasattr(result, \"__len__\") and not isinstance(result, (str, bytes, dict)):\n",
    "            try:\n",
    "                print(\"len:\", len(result))\n",
    "                # If this is a CalibratedExplanations collection, count non-None explanations\n",
    "                expls = getattr(result, \"explanations\", None)\n",
    "                if expls is not None:\n",
    "                    total = len(expls)\n",
    "                    explained = sum(1 for e in expls if e is not None)\n",
    "                    print(f\"explained_count: {explained} / {total}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        explained = sum(1 for e in result if e is not None)\n",
    "                        print(f\"explained_count (iter): {explained} / {len(result)}\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            print(repr(result))\n",
    "        return\n",
    "    print(\"policy:\", result.policy)\n",
    "    rejected = getattr(result, \"rejected\", None)\n",
    "    if rejected is not None:\n",
    "        rejected = np.asarray(rejected, dtype=bool)\n",
    "        print(\"rejected_count:\", int(rejected.sum()), \"/\", int(rejected.size))\n",
    "    pred = getattr(result, \"prediction\", None)\n",
    "    print(\"has_prediction:\", pred is not None)\n",
    "    expl = getattr(result, \"explanation\", None)\n",
    "    if expl is None:\n",
    "        print(\"explanation: None\")\n",
    "    else:\n",
    "        if hasattr(expl, \"__len__\") and not isinstance(expl, (str, bytes, dict)):\n",
    "            print(\"explanations_len:\", len(expl))\n",
    "            for i, e in enumerate(expl):\n",
    "                if i >= 5:\n",
    "                    print(\"... (truncated)\")\n",
    "                    break\n",
    "                if e is None:\n",
    "                    print(f\" [{i}] None\")\n",
    "                    continue\n",
    "                if hasattr(e, \"prediction\"):\n",
    "                    try:\n",
    "                        p = e.prediction\n",
    "                        if isinstance(p, dict) and \"predict\" in p:\n",
    "                            print(f' [{i}] predict={p[\"predict\"]:.3f}')\n",
    "                        else:\n",
    "                            print(f\" [{i}] prediction_present\")\n",
    "                    except Exception:\n",
    "                        print(f\" [{i}] prediction (unprintable)\")\n",
    "                else:\n",
    "                    print(f\" [{i}] explanation_type={type(e)}\")\n",
    "        else:\n",
    "            print(\"explanation_type:\", type(expl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "922d71f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- predict_internal(reject_policy=none) ---\n",
      "type: <class 'tuple'>\n",
      "len: 4\n",
      "explained_count (iter): 3 / 4\n",
      "\n",
      "--- predict_internal(reject_policy=predict_and_flag) ---\n",
      "type: <class 'calibrated_explanations.explanations.reject.RejectResult'>\n",
      "policy: RejectPolicy.PREDICT_AND_FLAG\n",
      "rejected_count: 0 / 4\n",
      "has_prediction: True\n",
      "explanation: None\n",
      "\n",
      "--- predict_internal(reject_policy=explain_all) ---\n",
      "type: <class 'calibrated_explanations.explanations.reject.RejectResult'>\n",
      "policy: RejectPolicy.EXPLAIN_ALL\n",
      "rejected_count: 1 / 4\n",
      "has_prediction: True\n",
      "explanations_len: 4\n",
      " [0] explanation_type=<class 'numpy.ndarray'>\n",
      " [1] explanation_type=<class 'numpy.ndarray'>\n",
      " [2] explanation_type=<class 'numpy.ndarray'>\n",
      " [3] None\n",
      "\n",
      "--- predict_internal(reject_policy=explain_rejects) ---\n",
      "type: <class 'calibrated_explanations.explanations.reject.RejectResult'>\n",
      "policy: RejectPolicy.EXPLAIN_REJECTS\n",
      "rejected_count: 0 / 4\n",
      "has_prediction: True\n",
      "explanation: None\n",
      "\n",
      "--- predict_internal(reject_policy=explain_non_rejects) ---\n",
      "type: <class 'calibrated_explanations.explanations.reject.RejectResult'>\n",
      "policy: RejectPolicy.EXPLAIN_NON_REJECTS\n",
      "rejected_count: 1 / 4\n",
      "has_prediction: True\n",
      "explanations_len: 4\n",
      " [0] explanation_type=<class 'numpy.ndarray'>\n",
      " [1] explanation_type=<class 'numpy.ndarray'>\n",
      " [2] explanation_type=<class 'numpy.ndarray'>\n",
      " [3] None\n",
      "\n",
      "--- predict_internal(reject_policy=skip_on_reject) ---\n",
      "type: <class 'calibrated_explanations.explanations.reject.RejectResult'>\n",
      "policy: RejectPolicy.SKIP_ON_REJECT\n",
      "rejected_count: 1 / 4\n",
      "has_prediction: True\n",
      "explanations_len: 4\n",
      " [0] explanation_type=<class 'numpy.ndarray'>\n",
      " [1] explanation_type=<class 'numpy.ndarray'>\n",
      " [2] explanation_type=<class 'numpy.ndarray'>\n",
      " [3] None\n",
      "\n",
      "--- explain_factual(reject_policy=none) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n",
      "\n",
      "--- explain_factual(reject_policy=predict_and_flag) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n",
      "\n",
      "--- explain_factual(reject_policy=explain_all) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n",
      "\n",
      "--- explain_factual(reject_policy=explain_rejects) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n",
      "\n",
      "--- explain_factual(reject_policy=explain_non_rejects) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n",
      "\n",
      "--- explain_factual(reject_policy=skip_on_reject) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n",
      "\n",
      "--- explain_factual(default_reject_policy=EXPLAIN_REJECTS) ---\n",
      "type: <class 'calibrated_explanations.explanations.explanations.CalibratedExplanations'>\n",
      "len: 4\n",
      "explained_count: 4 / 4\n"
     ]
    }
   ],
   "source": [
    "from calibrated_explanations.core.reject.policy import RejectPolicy\n",
    "\n",
    "POLICIES = [\n",
    "    RejectPolicy.NONE,\n",
    "    RejectPolicy.PREDICT_AND_FLAG,\n",
    "    RejectPolicy.EXPLAIN_ALL,\n",
    "    RejectPolicy.EXPLAIN_REJECTS,\n",
    "    RejectPolicy.EXPLAIN_NON_REJECTS,\n",
    "    RejectPolicy.SKIP_ON_REJECT,\n",
    "]\n",
    "# Policy-first demo: show predict_internal outputs for each policy\n",
    "for policy in POLICIES:\n",
    "    res = ce.predict_internal(X_test[:4], reject_policy=policy)\n",
    "    _summarize_result(res, label=f\"predict_internal(reject_policy={policy.value})\")\n",
    "\n",
    "# Then show explain_factual with each policy (deterministic plugin off)\n",
    "for policy in POLICIES:\n",
    "    res = ce.explain_factual(X_test[:4], reject_policy=policy, _use_plugin=False)\n",
    "    _summarize_result(res, label=f\"explain_factual(reject_policy={policy.value})\")\n",
    "\n",
    "# Show explainer-level default policy behaviour\n",
    "ce.default_reject_policy = RejectPolicy.EXPLAIN_REJECTS\n",
    "res_def = ce.explain_factual(X_test[:4], _use_plugin=False)\n",
    "_summarize_result(res_def, label=\"explain_factual(default_reject_policy=EXPLAIN_REJECTS)\")\n",
    "ce.default_reject_policy = RejectPolicy.NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa860f08",
   "metadata": {},
   "source": [
    "## Legacy: `predict_reject`-based triage (legacy behaviour)\n",
    "The examples below show the older workflow that explicitly calls `predict_reject` and then performs manual triage/explanations.\n",
    "This legacy path is fully supported but the policy-first examples above are the recommended pattern going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76ae8038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected (first 10): [True, False, False, False, False, False, False, False, False, True]\n",
      "reject_rate=0.105, error_rate=0.056\n",
      "Instance 0: Rejected - requires human review  Probability: 0.860\n",
      "Instance 1: Auto-decided - confident prediction  Probability: 0.026\n",
      "Instance 2: Auto-decided - confident prediction  Probability: 0.026\n",
      "Instance 3: Auto-decided - confident prediction  Probability: 0.960\n",
      "Instance 4: Auto-decided - confident prediction  Probability: 0.960\n",
      "Instance 5: Auto-decided - confident prediction  Probability: 0.026\n",
      "Instance 6: Auto-decided - confident prediction  Probability: 0.026\n",
      "Instance 7: Auto-decided - confident prediction  Probability: 0.125\n",
      "Instance 8: Auto-decided - confident prediction  Probability: 0.026\n",
      "Instance 9: Rejected - requires human review  Probability: 0.847\n"
     ]
    }
   ],
   "source": [
    "# Legacy example using predict_reject\n",
    "rejected_mask, error_rate, reject_rate = ce.predict_reject(X_test, confidence=0.95)\n",
    "rejected_mask = np.asarray(rejected_mask, dtype=bool)\n",
    "print(\"Rejected (first 10):\", rejected_mask[:10].tolist())\n",
    "print(f\"reject_rate={reject_rate:.3f}, error_rate={error_rate:.3f}\")\n",
    "\n",
    "# Triage loop (legacy)\n",
    "for i in range(min(10, len(y_test))):\n",
    "    if rejected_mask[i]:\n",
    "        print(f\"Instance {i}: Rejected - requires human review\", end=\"\")\n",
    "        explanations = ce.explain_factual(X_test[i : i + 1], _use_plugin=False)\n",
    "        print(f'  Probability: {explanations[0].prediction[\"predict\"]:.3f}')\n",
    "    else:\n",
    "        print(f\"Instance {i}: Auto-decided - confident prediction\", end=\"\")\n",
    "        pred = ce.predict_proba(X_test[i : i + 1])\n",
    "        print(f\"  Probability: {pred[0][1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibrated_explanations_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
