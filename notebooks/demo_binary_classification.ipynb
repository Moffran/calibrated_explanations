{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrated Explanations for Binary Classification\n",
    "## Demonstrated on the diabetes data set\n",
    "\n",
    "Author: Tuwe Löfström (tuwe.lofstrom@ju.se)  \n",
    "Copyright 2023 Tuwe Löfström  \n",
    "License: BSD 3 clause\n",
    "Sources:\n",
    "1. [Pima Indians Diabetes Database [kaggle]](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n",
    "2. [\"Calibrated Explanations: with Uncertainty Information and Counterfactuals\"](https://arxiv.org/abs/2305.02305) by [Helena Löfström](https://github.com/Moffran), [Tuwe Löfström](https://github.com/tuvelofstrom), Ulf Johansson, and Cecilia Sönströd.\n",
    "\n",
    "### 1. Import packages, data and train an underlying model\n",
    "#### 1.1 Import packages\n",
    "\n",
    "In the examples below, we will be using `NumPy`, `pandas`, and `sklearn`.  `CalibratedExplainer` and `VennAbers` are imported from `calibrated_explanations`. `VennAbers` is used to demonstrate how it can be used to calibrate an underlying model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from calibrated_explanations import CalibratedExplainer, VennAbers, __version__\n",
    "\n",
    "print(f\"calibrated_explanations v. {__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import data and train a model\n",
    "Let us import the Califronia Housing data set (see sources at the top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = 'diabetes_full'\n",
    "delimiter = ','\n",
    "model = 'RF'\n",
    "print(dataSet)\n",
    "\n",
    "fileName = '../data/' + dataSet + \".csv\"\n",
    "df = pd.read_csv(fileName, delimiter=delimiter)\n",
    "target = 'Y'\n",
    "X, y = df.drop(target,axis=1), df[target] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data set into a training and a test set, and further split the training set into a proper training set and a calibration set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_test = 2 # number of instances to test, one from each class\n",
    "\n",
    "no_of_classes = len(np.unique(y))\n",
    "no_of_features = X.shape[1]\n",
    "no_of_instances = X.shape[0]\n",
    "\n",
    "# find categorical features\n",
    "categorical_features = [i for i in range(no_of_features) if len(np.unique(X.iloc[:,i])) < 10]\n",
    "\n",
    "# select test instances from each class and split into train, cal and test\n",
    "idx = np.argsort(y.values).astype(int)\n",
    "X, y = X.values[idx,:], y.values[idx]\n",
    "test_index = np.array([*range(int(num_to_test/2)), *range(no_of_instances-1, no_of_instances-int(num_to_test/2)-1,-1)])\n",
    "train_index = np.setdiff1d(np.array(range(no_of_instances)), test_index)   \n",
    "trainCalX, testX = X[train_index,:], X[test_index,:]\n",
    "trainCalY, testY = y[train_index], y[test_index]\n",
    "trainX, calX, trainY, calY = train_test_split(trainCalX, trainCalY, test_size=0.33,random_state=42, stratify=trainCalY)\n",
    "\n",
    "print(testY)\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a random forest to the proper training set. We also set a random seed to be able to rerun the notebook and get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(trainX,trainY)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting an explanation, lets see what the output from the model and from Venn-Abers (va) are for the test instances.\n",
    "\n",
    "For an explanation of Venn-Abers, see Paulo Toccacellis Github repo [VennABERS](https://github.com/ptocca/VennABERS) or read the [\"Calibrated Explanations: with Uncertainty Information and Counterfactuals\"](https://arxiv.org/abs/2305.02305) paper introducing this package.\n",
    "\n",
    "Outputs are probabilities for `class #1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(testX)\n",
    "proba = model.predict_proba(testX)\n",
    "\n",
    "va = VennAbers(calX, calY, model)  \n",
    "va_preds = va.predict(testX)\n",
    "va_proba, low, high = va.predict_proba(testX, output_interval=True)\n",
    "\n",
    "print(\"Model: pred (proba) - Va: pred (proba) [interval] - Y\")\n",
    "for pred, proba, va_pred, va_proba, lo, hi, y in zip(preds, proba, va_preds, va_proba, low, high, testY):\n",
    "    output_str = f\"Model: {pred} ({proba[1]:.2f}) - Va: {va_pred} ({va_proba[1]:.2f}) [{lo:.2f}-{hi:.2f}] - {y}\"\n",
    "    print(output_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `CalibratedExplainer` by feeding the model and the calibration set as a minimum (mode is 'classification' by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = CalibratedExplainer(model, \n",
    "                         calX, \n",
    "                         calY,\n",
    "                         feature_names=df.columns,                    \n",
    "                         categorical_features=categorical_features, \n",
    "                         class_labels={0:'Non-diabetic',1:'Diabetic'})\n",
    "display(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular and uncertainty explanations\n",
    "When using regular or uncertainty plots, the recommended (and for classification default) `Discretizer` is the `BinaryEntropyDiscretizer`. As no discretizer was assigned at initialization, it is already assigned `BinaryEntropyDiscretizer`. \n",
    "\n",
    "Once the explanations are extracted, we can visualize them using regular or uncertainty plots. The regular plot include an uncertainty interval for each class and the weights of the most influential features. The weights (positive or negative) always indicate the impact on the blue class. However, the colors are used to indicate which class is positively affected. Negative (red) weights are reducing the probability of the blue class and increasing the probability of the red class. \n",
    "\n",
    "Regular plots are shown by calling the function `plot_regular`, with `n_features_to_show` indicating the number of features to include, in order of importance. To save the plots to disk, `save_ext` can take one or several of the following extensions `['pdf','svg','png']` creating a plot for each instance and file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ce(testX)\n",
    "exp.plot_regular(n_features_to_show=10, save_ext=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty plots are similar to regular plots but also provide an uncertainty estimate for the impact of each feature. Here, the shaded area is the range of possible changes that each feature can result in.\n",
    "\n",
    "To get uncertainty plots, the `plot_uncertainty` function can be called with the same parameters as `plot_regular`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.plot_uncertainty(n_features_to_show=10, save_ext=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunctional rules\n",
    "In the examples above, each explanation only contained atomic rules, including a single feature. It is also possible to combine rules and see the combined impact of more than one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.add_conjunctive_factual_rules()\n",
    "exp.plot_regular(n_features_to_show=15, save_ext=[])\n",
    "exp.plot_uncertainty(n_features_to_show=15, save_ext=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counterfactual explanations\n",
    "When using counterfactual explanations, the recommended `Discretizer` is the `EntropyDiscretizer`. The discretizer can be changed into `EntropyDiscretizer` by invoking `set_discretizer('entropy')`. Counterfactual explanations are extracted in the same way as regular and uncertainty explanations. The `EntropyDiscretizer` allows for more varied and precise counterfactual rules. \n",
    "\n",
    "Once the explanations are extracted, we can visualize them using `plot_counterfactuals` plot. The cpunterfactual plot visualize the probability interval for the positive class (`Diabetic` in this example). The counterfactual rules indicate what the interval would have changed into had the feature values changed according to the rule condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = 'entropy'\n",
    "ce.set_discretizer(discretizer)\n",
    "\n",
    "exp = ce(testX)\n",
    "exp.plot_counterfactuals(n_features_to_show=10, save_ext=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunctional counterfactual explanations\n",
    "As with regular and uncertainty explanations, conjunctions can also be used for counterfactual explanations.\n",
    "\n",
    "As the `add_conjunctive...` functions return the explanation object, they can be stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.add_conjunctive_counterfactual_rules().plot_counterfactuals(n_features_to_show=15, save_ext=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "168dd73c7a7b76a0355e35f33a90e68c167b1dbb1e524891be00dd5c7b8524eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
