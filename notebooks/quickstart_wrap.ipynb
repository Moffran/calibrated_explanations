{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with calibrated-explanations - using the WrapCalibratedExplainer class\n",
    "Author: Tuwe Löfström (tuwe.lofstrom@ju.se)  \n",
    "Copyright 2023 Tuwe Löfström  \n",
    "License: BSD 3 clause\n",
    "## Classification\n",
    "Let us illustrate how we may use `calibrated_explanations` to generate explanations from a classifier trained on a dataset from\n",
    "[www.openml.org](https://www.openml.org), which we first split into a\n",
    "training and a test set using `train_test_split` from\n",
    "[sklearn](https://scikit-learn.org), and then further split the\n",
    "training set into a proper training set and a calibration set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = fetch_openml(name=\"wine\", version=7, as_frame=True, parser='auto')\n",
    "\n",
    "X = dataset.data.values.astype(float)\n",
    "y = (dataset.target.values == 'True').astype(int)\n",
    "\n",
    "feature_names = dataset.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2, stratify=y)\n",
    "\n",
    "X_prop_train, X_cal, y_prop_train, y_cal = train_test_split(X_train, y_train,\n",
    "                                                            test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our wrapper object, using a `RandomForestClassifier` as learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from calibrated_explanations import WrapCalibratedExplainer, __version__\n",
    "\n",
    "print(f\"calibrated_explanations {__version__}\")\n",
    "\n",
    "classifier = WrapCalibratedExplainer(RandomForestClassifier())\n",
    "display(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit our model using the proper training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_prop_train, y_prop_train)\n",
    "display(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WrapCalibratedExplainer` class has a `predict` and a `predict_proba` method that returns the predictions and probability estimates of the underlying classifier. If the model is not yet calibrated, then the underlying models `predict` and `predict_proba` methods are used. If the model is calibrated, then the `predict` and `predict_proba` method of the calibration model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uncalibrated probability estimates: \\n{classifier.predict_proba(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can generate explanations, we need to calibrate our model using the calibration set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.calibrate(X_cal, y_cal, feature_names=feature_names)\n",
    "display(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is calibrated, the `predict` and `predict_proba` methods produce calibrated predictions and probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba, (low, high) = classifier.predict_proba(X_test, uq_interval=True)\n",
    "print(f'Calibrated probability estimates: \\n{proba}')\n",
    "print(f'Calibrated uncertainty interval for the positive class: [{[(low[i], high[i]) for i in range(len(low))]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factual Explanations\n",
    "Let us explain a test instance using our `WrapCalibratedExplainer` object. The method used to get factual explanations is `explain_factual`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_explanations = classifier.explain_factual(X_test)\n",
    "display(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the explanations, we can plot all of them using `plot_all`. Default, a regular plot, without uncertainty intervals included, is created. To include uncertainty intervals, change the parameter `uncertainty=True`. To plot only a single instance, the `plot_explanation` function can be called, submitting the index of the test instance to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_explanations.plot_all()\n",
    "factual_explanations.plot_all(uncertainty=True)\n",
    "\n",
    "factual_explanations.plot_explanation(0, uncertainty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add and remove conjunctive rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_explanations.add_conjunctions().plot_explanation(0)\n",
    "factual_explanations.plot_explanation(0, uncertainty=True)\n",
    "factual_explanations.remove_conjunctions().plot_explanation(0, uncertainty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counterfactual Explanations\n",
    "An alternative to factual rules is to extract counterfactual rules. \n",
    "`explain_counterfactual` can be called to get counterfactual rules with an appropriate discretizer automatically assigned. Note that the discretizer has changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_explanations = classifier.explain_counterfactual(X_test)\n",
    "display(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counterfactuals are also visualized using the `plot_all`. Plotting an individual counterfactual explanation is done using `plot_explanation`, submitting the index to plot. Adding or removing conjunctions is done as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_explanations.plot_all()\n",
    "counterfactual_explanations.add_conjunctions().plot_all()\n",
    "\n",
    "counterfactual_explanations.plot_explanation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calibrated_explanations` supports multiclass which is demonstrated in [demo_multiclass](https://github.com/Moffran/calibrated_explanations/blob/main/notebooks/demo_multiclass.ipynb). That notebook also demonstrates how both feature names and target and categorical labels can be added to improve the interpretability. \n",
    "## Regression\n",
    "Extracting explanations for regression is very similar to how it is done for classification. First we load and divide the dataset. The target is divided by 1000, meaning that the target is in thousands of dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_openml(name=\"house_sales\", version=3)\n",
    "\n",
    "X = dataset.data.values.astype(float)\n",
    "y = dataset.target.values/1000\n",
    "y_filter = y < 400\n",
    "X = X[y_filter,:]\n",
    "y = y[y_filter]\n",
    "\n",
    "feature_names = dataset.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2, random_state=42)\n",
    "\n",
    "X_prop_train, X_cal, y_prop_train, y_cal = train_test_split(X_train, y_train,\n",
    "                                                            test_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our wrapper object, using a `RandomForestRegressor` as learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = WrapCalibratedExplainer(RandomForestRegressor())\n",
    "display(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit our model using the proper training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_prop_train, y_prop_train)\n",
    "display(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WrapCalibratedExplainer` class has a `predict` method that returns the predictions and probability estimates of the underlying classifier. If the model is not yet calibrated, then the underlying models `predict` method is used. If the model is calibrated, then the `predict` method of the calibration model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uncalibrated model prediction: \\n{regressor.predict(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can generate explanations, we need to calibrate our model using the calibration set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.calibrate(X_cal, y_cal, feature_names=feature_names)\n",
    "display(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily add a difficulty estimator by assigning a `DifficultyEstimator` to the `difficulty_estimator` attribute when calibrating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crepes.extras import DifficultyEstimator\n",
    "\n",
    "regressor.calibrate(X_cal, y_cal, feature_names=feature_names, \n",
    "                    difficulty_estimator=DifficultyEstimator().fit(X=X_prop_train, learner=regressor.learner, scaler=True))\n",
    "display(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is calibrated, the `predict`  method produce calibrated predictions with uncertainties. The default confidence is 90 per cent, which can be altered using the `low_high_percentiles` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, (low, high) = regressor.predict(X_test, uq_interval=True, low_high_percentiles=(5, 95))\n",
    "print(f'Calibrated prediction: \\n{prediction}')\n",
    "print(f'Calibrated uncertainty interval: [{[(low[i], high[i]) for i in range(len(low))]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the probability of the prediction being below a certain threshold using `predict_proba` by assigning the `threshold` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = regressor.predict(X_test, threshold=200)\n",
    "print(f'Calibrated probabilistic prediction: {prediction}')\n",
    "proba, (low, high) = regressor.predict_proba(X_test, uq_interval=True, threshold=200)\n",
    "print(f'Calibrated probabilistic probability estimate [y_hat > threshold, y_hat <= threshold]: \\n{proba}')\n",
    "print(f'Calibrated probabilistic uncertainty interval for y_hat <= threshold: [{[(low[i], high[i]) for i in range(len(low))]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factual Explanations\n",
    "Let us explain a test instance using our `WrapCalibratedExplainer` object. The method used to get factual explanations is `explain_factual`. Note that the discretizer is now assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_explanations = regressor.explain_factual(X_test)\n",
    "display(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression also offer both regular and uncertainty plots for factual explanations with or without conjunctive rules, in almost exactly the same way as for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_explanations.plot_all()\n",
    "factual_explanations.plot_all(uncertainty=True)\n",
    "\n",
    "factual_explanations.add_conjunctions().plot_all(uncertainty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counterfactual Explanations\n",
    "The `explain_counterfactual` will work exactly the same as for classification. Note that the discretizer is now changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_explanations = regressor.explain_counterfactual(X_test)\n",
    "display(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counterfactual plots work as for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_explanations.plot_all()\n",
    "counterfactual_explanations.add_conjunctions().plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Regression\n",
    "The difference between probabilistic regression and regular regression is that the former returns a probability of the prediction being below a certain threshold. This could for example be useful when the prediction is a time to an event, such as time to death or time to failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_factual_explanations = regressor.explain_factual(X_test, threshold=200)\n",
    "probabilistic_factual_explanations.plot_all()\n",
    "probabilistic_factual_explanations.plot_all(uncertainty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_counterfactual_explanations = regressor.explain_counterfactual(X_test, threshold=200)\n",
    "probabilistic_counterfactual_explanations.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression offers many more options but to learn more about them, see the [demo_regression](https://github.com/Moffran/calibrated_explanations/blob/main/notebooks/demo_regression.ipynb) or the [demo_probabilistic_regression](https://github.com/Moffran/calibrated_explanations/blob/main/notebooks/demo_probabilistic_regression.ipynb) notebooks.\n",
    "\n",
    "A `WrapCalibratedExplainer` can also be initialized with a trained model or with a `CalibratedExplainer` object, as is examplified below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classifier = WrapCalibratedExplainer(classifier.learner)\n",
    "display(new_classifier)\n",
    "new_classifier_explainer = WrapCalibratedExplainer(classifier.explainer)\n",
    "display(new_classifier_explainer)\n",
    "\n",
    "new_regressor = WrapCalibratedExplainer(regressor.learner)\n",
    "display(new_regressor)\n",
    "new_regressor_explainer = WrapCalibratedExplainer(regressor.explainer)\n",
    "display(new_regressor_explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a calibrated explainer is re-fitted, the explainer is reinitialized."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e14fe98f360bed4093bab26da9581d5c2feb5ee20210386a1ced5a5e3c396d00"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
